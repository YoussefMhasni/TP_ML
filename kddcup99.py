# -*- coding: utf-8 -*-
"""KDDCup99.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uevb3XgONpA_guIEirCUnr5V5fCtNy0s
"""

import pandas as pd
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import classification_report, f1_score
import numpy as np

# Chargement des données KDDCup99
file_path_kddcup99 = 'KDDCup99.csv'
data = pd.read_csv(file_path_kddcup99)

data.head(50)

data.describe()

data.info()

# Identifier les colonnes catégorielles
categorical_columns = data.select_dtypes(include=['object', 'category']).columns

# Prétraitement des données
if len(categorical_columns) > 0:
    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', OneHotEncoder(), categorical_columns)
        ],
        remainder='passthrough'
    )
else:
    preprocessor = 'passthrough'

# Pipeline de prétraitement et de modèle
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('scaler', StandardScaler()),
    ('model', None)
])

# Séparation en caractéristiques (features) et étiquettes (labels)
X = data.drop('label', axis=1)
y = data['label']

# Séparation en données d'entraînement et de test avec échantillonnage stratifié
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Model definitions
models = {
    "Isolation Forest": IsolationForest(),
    "Local Outlier Factor": LocalOutlierFactor(novelty=True)
}

# Parameters for grid search
param_grid = {
    "Isolation Forest": {"model__n_estimators": [50, 100, 200]},
    "Local Outlier Factor": {"model__n_neighbors": [5, 20, 35]}
}

# Cross-validation setup
cv = KFold(n_splits=5)

# Training and optimizing models
best_models = {}
for name, model in models.items():
    pipeline.set_params(model=model)
    grid_search = GridSearchCV(pipeline, param_grid[name], cv=cv, scoring='f1_macro')
    grid_search.fit(X_train, y_train)
    best_models[name] = grid_search.best_estimator_

# Model evaluation using F1 score
for name, best_model in best_models.items():
    y_pred = best_model.predict(X_test)
    print(f"Classification report for {name}:")
    print(classification_report(y_test, y_pred))
    print(f"F1 Score for {name}: {f1_score(y_test, y_pred, average='macro')}")

# Select normal data for training novelty detection models
X_train_normal = X_train[y_train == 0]

# Retraining models for novelty detection
models_novelty = {
    "Isolation Forest": IsolationForest(n_estimators=best_models["Isolation Forest"].named_steps['model'].n_estimators),
    "Local Outlier Factor": LocalOutlierFactor(n_neighbors=best_models["Local Outlier Factor"].named_steps['model'].n_neighbors, novelty=True)
}

for name, model in models_novelty.items():
    model.fit(X_train_normal)
    y_pred = model.predict(X_test)
    # Adjust predictions for Local Outlier Factor if necessary
    if name == "Local Outlier Factor":
        y_pred = np.where(y_pred == 1, 0, 1)  # Inverting predictions for LOF
    else:
        y_pred = np.where(y_pred == 1, 0, 1)  # Isolation Forest returns -1 for anomalies

    # Evaluation and comparison of performance
    print(f"\nClassification report for novelty detection with {name}:")
    print(classification_report(y_test, y_pred))
    print(f"F1 Score for novelty detection with {name}: {f1_score(y_test, y_pred, average='macro')}")